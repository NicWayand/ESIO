{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "This code is part of the SIPN2 project focused on improving sub-seasonal to seasonal predictions of Arctic Sea Ice. \n",
    "If you use this code for a publication or presentation, please cite the reference in the README.md on the\n",
    "main page (https://github.com/NicWayand/ESIO). \n",
    "\n",
    "Questions or comments should be addressed to nicway@uw.edu\n",
    "\n",
    "Copyright (c) 2018 Nic Wayand\n",
    "\n",
    "GNU General Public License v3.0\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "Plot forecast maps with all available models.\n",
    "'''\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import struct\n",
    "import os\n",
    "import xarray as xr\n",
    "import glob\n",
    "import datetime\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import seaborn as sns\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning) # not good to supress but they divide by nan are annoying\n",
    "#warnings.simplefilter(action='ignore', category=UserWarning) # https://github.com/pydata/xarray/issues/2273\n",
    "import json\n",
    "from esio import EsioData as ed\n",
    "from esio import ice_plot\n",
    "from esio import import_data\n",
    "import subprocess\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x7fc5d9755470>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask.config.set(scheduler='threads')  # overwrite default with threaded scheduler (This is faster for this code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import Client\n",
    "# client = Client(n_workers=8)\n",
    "# client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-07T00:00:00.000000000 2018-12-23T00:00:00.000000000\n"
     ]
    }
   ],
   "source": [
    "#def Update_PanArctic_Maps():\n",
    "# Plotting Info\n",
    "runType = 'forecast'\n",
    "variables = ['sic']\n",
    "metrics_all = {'sic':['anomaly','mean','SIP'], 'hi':['mean']}\n",
    "#metrics_all = {'sic':['SIP']}\n",
    "updateAll = False\n",
    "\n",
    "# Define Init Periods here, spaced by 7 days (aprox a week)\n",
    "# Now\n",
    "cd = datetime.datetime.now()\n",
    "cd = datetime.datetime(cd.year, cd.month, cd.day) # Set hour min sec to 0. \n",
    "# Hardcoded start date (makes incremental weeks always the same)\n",
    "start_t = datetime.datetime(1950, 1, 1) # datetime.datetime(1950, 1, 1)\n",
    "# Params for this plot\n",
    "Ndays = 7 # time period to aggregate maps to (default is 7)\n",
    "Npers = 4 # init periods to put into a Zarr chunk\n",
    "\n",
    "init_start_date = np.datetime64('2018-01-01')\n",
    "\n",
    "init_slice = np.arange(start_t, cd, datetime.timedelta(days=Ndays)).astype('datetime64[ns]')\n",
    "# init_slice = init_slice[-Npers:] # Select only the last Npers of periods (weeks) since current date\n",
    "init_slice = init_slice[init_slice>=init_start_date] # Select only the inits after init_start_date\n",
    "print(init_slice[0],init_slice[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2018-01-07T00:00:00.000000000', '2018-01-14T00:00:00.000000000',\n",
       "       '2018-01-21T00:00:00.000000000', '2018-01-28T00:00:00.000000000',\n",
       "       '2018-02-04T00:00:00.000000000', '2018-02-11T00:00:00.000000000',\n",
       "       '2018-02-18T00:00:00.000000000', '2018-02-25T00:00:00.000000000',\n",
       "       '2018-03-04T00:00:00.000000000', '2018-03-11T00:00:00.000000000',\n",
       "       '2018-03-18T00:00:00.000000000', '2018-03-25T00:00:00.000000000',\n",
       "       '2018-04-01T00:00:00.000000000', '2018-04-08T00:00:00.000000000',\n",
       "       '2018-04-15T00:00:00.000000000', '2018-04-22T00:00:00.000000000',\n",
       "       '2018-04-29T00:00:00.000000000', '2018-05-06T00:00:00.000000000',\n",
       "       '2018-05-13T00:00:00.000000000', '2018-05-20T00:00:00.000000000',\n",
       "       '2018-05-27T00:00:00.000000000', '2018-06-03T00:00:00.000000000',\n",
       "       '2018-06-10T00:00:00.000000000', '2018-06-17T00:00:00.000000000',\n",
       "       '2018-06-24T00:00:00.000000000', '2018-07-01T00:00:00.000000000',\n",
       "       '2018-07-08T00:00:00.000000000', '2018-07-15T00:00:00.000000000',\n",
       "       '2018-07-22T00:00:00.000000000', '2018-07-29T00:00:00.000000000',\n",
       "       '2018-08-05T00:00:00.000000000', '2018-08-12T00:00:00.000000000',\n",
       "       '2018-08-19T00:00:00.000000000', '2018-08-26T00:00:00.000000000',\n",
       "       '2018-09-02T00:00:00.000000000', '2018-09-09T00:00:00.000000000',\n",
       "       '2018-09-16T00:00:00.000000000', '2018-09-23T00:00:00.000000000',\n",
       "       '2018-09-30T00:00:00.000000000', '2018-10-07T00:00:00.000000000',\n",
       "       '2018-10-14T00:00:00.000000000', '2018-10-21T00:00:00.000000000',\n",
       "       '2018-10-28T00:00:00.000000000', '2018-11-04T00:00:00.000000000',\n",
       "       '2018-11-11T00:00:00.000000000', '2018-11-18T00:00:00.000000000',\n",
       "       '2018-11-25T00:00:00.000000000', '2018-12-02T00:00:00.000000000',\n",
       "       '2018-12-09T00:00:00.000000000', '2018-12-16T00:00:00.000000000',\n",
       "       '2018-12-23T00:00:00.000000000'], dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# Load in Observed and non-dynamic model Data\n",
    "#############################################################\n",
    "\n",
    "E = ed.EsioData.load()\n",
    "mod_dir = E.model_dir\n",
    "\n",
    "# # Get median ice edge by DOY\n",
    "# median_ice_fill = xr.open_mfdataset(os.path.join(E.obs_dir, 'NSIDC_0051', 'agg_nc', 'ice_edge.nc')).sic\n",
    "# # Get mean sic by DOY\n",
    "# mean_1980_2010_sic = xr.open_dataset(os.path.join(E.obs_dir, 'NSIDC_0051', 'agg_nc', 'mean_1980_2010_sic.nc')).sic\n",
    "# # Get average sip by DOY\n",
    "# mean_1980_2010_SIP = xr.open_dataset(os.path.join(E.obs_dir, 'NSIDC_0051', 'agg_nc', 'hist_SIP_1980_2010.nc')).sip    \n",
    "\n",
    "# # Get recent observations\n",
    "# ds_81 = xr.open_mfdataset(E.obs['NSIDC_0081']['sipn_nc']+'_yearly/*.nc', concat_dim='time', autoclose=True, parallel=True)#,\n",
    "\n",
    "# # Define models to plot\n",
    "# models_2_plot = list(E.model.keys())\n",
    "# models_2_plot = [x for x in models_2_plot if x not in ['piomas','MME','MME_NEW','uclsipn','hcmr']] # remove some models\n",
    "# models_2_plot = [x for x in models_2_plot if E.icePredicted[x]] # Only predictive models\n",
    "# # models_2_plot = ['rasmesrl']\n",
    "# models_2_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking on  2018-01-07T00:00:00.000000000\n",
      "Checking on  2018-02-04T00:00:00.000000000\n",
      "Checking on  2018-03-04T00:00:00.000000000\n",
      "Checking on  2018-04-01T00:00:00.000000000\n",
      "Checking on  2018-04-29T00:00:00.000000000\n",
      "Checking on  2018-05-27T00:00:00.000000000\n",
      "Checking on  2018-06-24T00:00:00.000000000\n",
      "Checking on  2018-07-22T00:00:00.000000000\n",
      "Checking on  2018-08-19T00:00:00.000000000\n",
      "Checking on  2018-09-16T00:00:00.000000000\n",
      "Checking on  2018-10-14T00:00:00.000000000\n",
      "Checking on  2018-11-11T00:00:00.000000000\n",
      "Processing 2018-11-11T00:00:00.000000000 2018-12-02T00:00:00.000000000\n",
      "Loading in weekly metrics...\n",
      "    Loading anomaly ...\n",
      "    Found 51 initialization periods.\n",
      "    Loading mean ...\n",
      "    Found 51 initialization periods.\n",
      "    Loading SIP ...\n",
      "    Found 51 initialization periods.\n",
      "Saving to Zarr...\n",
      "Finished  2018-11-11\n",
      "Took  1.810260215983726  minutes.\n",
      "Checking on  2018-12-09T00:00:00.000000000\n",
      "Processing 2018-12-09T00:00:00.000000000 2018-12-23T00:00:00.000000000\n",
      "Loading in weekly metrics...\n",
      "    Loading anomaly ...\n",
      "    Found 51 initialization periods.\n",
      "    Loading mean ...\n",
      "    Found 51 initialization periods.\n",
      "    Loading SIP ...\n",
      "    Found 51 initialization periods.\n",
      "Saving to Zarr...\n",
      "Finished  2018-12-09\n",
      "Took  1.2248599254370978  minutes.\n"
     ]
    }
   ],
   "source": [
    "cvar = 'sic' # hard coded for now\n",
    "\n",
    "\n",
    "# For each init chunk\n",
    "for IS in np.arange(0,len(init_slice),Npers): # Every fourth init date\n",
    "    start_time_cmod = timeit.default_timer()\n",
    "\n",
    "    it_start = init_slice[IS]\n",
    "    if (IS+Npers-1)>=len(init_slice):\n",
    "        it_end = init_slice[-1]\n",
    "    else:\n",
    "        it_end = init_slice[IS+Npers-1]\n",
    "    \n",
    "    # Output Zarr dir\n",
    "    c_zarr_file = os.path.join(E.data_dir,'model/zarr', 'temp', cvar+pd.to_datetime(it_start).strftime('_%Y-%m-%d')+'.zarr')\n",
    "    \n",
    "    print(\"Checking on \",it_start)\n",
    "    \n",
    "    # Check if dir exists\n",
    "    if updateAll | (os.path.isdir(c_zarr_file)==False) | (it_start>init_slice[-1] - np.timedelta64(60,'D')):\n",
    "\n",
    "        print(\"Processing\",it_start, it_end)\n",
    "\n",
    "        # Load in all metrics for given variable\n",
    "        print(\"Loading in weekly metrics...\")\n",
    "        ds_m = import_data.load_MME_by_init_end(E=E, \n",
    "                                                runType=runType, \n",
    "                                                variable=cvar, \n",
    "                                                metrics=metrics_all[cvar],\n",
    "                                                init_range=[it_start,it_end])\n",
    "\n",
    "        # Drop models that we don't evaluate (i.e. monthly means)\n",
    "        models_keep = [x for x in ds_m.model.values if x not in ['noaasipn','modcansipns_3','modcansipns_4']]\n",
    "        ds_m = ds_m.sel(model=models_keep)\n",
    "        # Get list of dynamical models that are not observations\n",
    "        dynamical_Models = [x for x in ds_m.model.values if x not in ['Observed','climatology','dampedAnomaly','dampedAnomalyTrend']]\n",
    "        # # Get list of all models\n",
    "        # all_Models = [x for x in ds_m.model.values if x not in ['Observed']]\n",
    "        # Add MME\n",
    "        MME_avg = ds_m.sel(model=dynamical_Models).mean(dim='model') # only take mean over dynamical models\n",
    "        MME_avg.coords['model'] = 'MME'\n",
    "        ds_ALL = xr.concat([ds_m, MME_avg], dim='model')\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "        ####################################\n",
    "#         print(ds_ALL)\n",
    "\n",
    "\n",
    "        # Rechunk from ~1MB to 100MB chunks\n",
    "        # Chunk along fore_time and init_end\n",
    "        ds_ALL = ds_ALL.chunk({'init_end':10,'fore_time': 10, 'model': 1, 'x': 304, 'y': 448})\n",
    "\n",
    "        # Save to Zarr chunk\n",
    "        print(\"Saving to Zarr...\")\n",
    "        ds_ALL.to_zarr(c_zarr_file, mode='w')\n",
    "        print(\"Finished \",pd.to_datetime(it_start).strftime('%Y-%m-%d'))\n",
    "        print(\"Took \", (timeit.default_timer() - start_time_cmod)/60, \" minutes.\")\n",
    "        ds_ALL=None # Flush memory\n",
    "        MME_avg=None\n",
    "        ds_m=None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xr.open_zarr('/home/disk/sipn/nicway/data/model/zarr/temp/sic_2018-06-24.zarr').SIP.sel(model='MME').notnull().sum().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sic_2018-01-07.zarr',\n",
       " 'sic_2018-02-04.zarr',\n",
       " 'sic_2018-03-04.zarr',\n",
       " 'sic_2018-04-01.zarr',\n",
       " 'sic_2018-04-29.zarr',\n",
       " 'sic_2018-05-27.zarr',\n",
       " 'sic_2018-06-24.zarr',\n",
       " 'sic_2018-07-22.zarr',\n",
       " 'sic_2018-08-19.zarr',\n",
       " 'sic_2018-09-16.zarr',\n",
       " 'sic_2018-10-14.zarr',\n",
       " 'sic_2018-11-11.zarr',\n",
       " 'sic_2018-12-09.zarr']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all Zarr chunks\n",
    "zarr_dir = '/home/disk/sipn/nicway/data/model/zarr/temp/'\n",
    "zarr_inits = sorted([ name for name in os.listdir(zarr_dir) if os.path.isdir(os.path.join(zarr_dir, name)) ])\n",
    "zarr_inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "zl = []\n",
    "for c_init in zarr_inits:\n",
    "    ds = xr.open_zarr(os.path.join(zarr_dir,c_init))\n",
    "    zl.append(ds)\n",
    "ds_Zarr = xr.concat(zl,dim='init_end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:    (fore_time: 52, init_end: 51, model: 20, x: 304, y: 448)\n",
      "Coordinates:\n",
      "  * model      (model) object 'MME' 'Observed' 'awispin' ... 'usnavysipn' 'yopp'\n",
      "  * fore_time  (fore_time) timedelta64[ns] 0 days 7 days ... 350 days 357 days\n",
      "    lat        (x, y) float64 31.1 31.25 31.4 31.55 ... 34.92 34.77 34.62 34.47\n",
      "    lon        (x, y) float64 168.3 168.4 168.5 168.7 ... -9.745 -9.872 -9.999\n",
      "  * x          (x) int64 0 1 2 3 4 5 6 7 8 ... 296 297 298 299 300 301 302 303\n",
      "  * y          (y) int64 0 1 2 3 4 5 6 7 8 ... 440 441 442 443 444 445 446 447\n",
      "  * init_end   (init_end) datetime64[ns] 2018-01-07 2018-01-14 ... 2018-12-23\n",
      "Data variables:\n",
      "    SIP        (init_end, model, fore_time, y, x) float64 dask.array<shape=(51, 20, 52, 448, 304), chunksize=(4, 1, 10, 448, 304)>\n",
      "    anomaly    (init_end, model, fore_time, y, x) float64 dask.array<shape=(51, 20, 52, 448, 304), chunksize=(4, 1, 10, 448, 304)>\n",
      "    mean       (init_end, model, fore_time, y, x) float64 dask.array<shape=(51, 20, 52, 448, 304), chunksize=(4, 1, 10, 448, 304)>\n"
     ]
    }
   ],
   "source": [
    "print(ds_Zarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### ADD METADATA #################\n",
    "\n",
    "## Add coordinate system info\n",
    "ds_Zarr.coords['crs'] = xr.DataArray('crs')\n",
    "ds_Zarr['crs'].attrs = {\n",
    "    'comment': '(https://nsidc.org/data/polar-stereo/ps_grids.html or https://nsidc.org/data/oib/epsg_3413.html) This is a container variable that describes the grid_mapping used by the data in this file. This variable does not contain any data; only information about the geographic coordinate system',\n",
    "    'grid_mapping_name': 'polar_stereographic',\n",
    "    'straight_vertical_longitude_from_pole':'-45',\n",
    "    'latitude_of_projection_origin': '90.0',\n",
    "    'standard_parallel':'70',\n",
    "    'false_easting':'0',\n",
    "    'false_northing':'0'\n",
    "    }\n",
    "\n",
    "# Add time coords\n",
    "ds_Zarr.coords['init_start'] = ds_Zarr.init_end - np.timedelta64(Ndays,'D') + np.timedelta64(1,'D')\n",
    "ds_Zarr['init_start'].attrs = {\n",
    "    'comment':        'Start date for weekly average period',\n",
    "    'long_name':      'Start date for weekly average period',\n",
    "    'standard_name':  \"start_init_date\"}\n",
    "\n",
    "ds_Zarr['init_end'].attrs = {\n",
    "    'comment':        'End date for weekly average period',\n",
    "    'long_name':      'End date for weekly average period',\n",
    "    'standard_name':  \"end_init_date\"}\n",
    "\n",
    "ds_Zarr['fore_time'].attrs = {\n",
    "    'comment':        'Forecast lead time',\n",
    "    'long_name':      'Forecast lead time',\n",
    "    'standard_name':  \"forecast_lead_time\"}\n",
    "\n",
    "# Add Valid time (start and end period)\n",
    "ds_Zarr = import_data.get_valid_time(ds_Zarr, init_dim='init_end', fore_dim='fore_time')\n",
    "ds_Zarr.rename({'valid_time':'valid_end'}, inplace=True);\n",
    "ds_Zarr.coords['valid_start'] = ds_Zarr.valid_end - np.timedelta64(Ndays,'D') + np.timedelta64(1,'D')\n",
    "\n",
    "# Add attributes\n",
    "ds_Zarr['valid_end'].attrs = {\n",
    "    'comment':        'End Valid date for weekly average period',\n",
    "    'long_name':      'End Valid date for weekly average period',\n",
    "    'standard_name':  \"end_valid_date\"}\n",
    "\n",
    "ds_Zarr['valid_start'].attrs = {\n",
    "    'comment':        'Start Valid date for weekly average period',\n",
    "    'long_name':      'Start Valid date for weekly average period',\n",
    "    'standard_name':  \"start_valid_date\"}\n",
    "\n",
    "# Add Variable attributes\n",
    "ds_Zarr['SIP'].attrs = {\n",
    "    'comment':        'Sea ice probability, calculated by averaging across ensemble members predictions of sea ice concentration >= 0.15',\n",
    "    'grid_mapping':   'crs',\n",
    "    'long_name':      'Sea ice probability',\n",
    "    'standard_name':  \"sea_ice_probability\",\n",
    "    'units':          'fraction'}\n",
    "\n",
    "ds_Zarr['anomaly'].attrs = {\n",
    "    'comment':        'Anomaly of the forecasted sea ice concentration mean (ensemble average) compared to the 1980 to 2010 Observed Climatology',\n",
    "    'grid_mapping':   'crs',\n",
    "    'long_name':      'Anomaly',\n",
    "    'standard_name':  \"anomaly\",\n",
    "    'units':          'fraction'}\n",
    "\n",
    "ds_Zarr['mean'].attrs = {\n",
    "    'comment':        'Mean of the forecasted sea ice concentration (ensemble average)',\n",
    "    'grid_mapping':   'crs',\n",
    "    'long_name':      'Sea ice concentration',\n",
    "    'standard_name':  \"sea_ice_concentration\",\n",
    "    'units':          'fraction'}\n",
    "\n",
    "# Dataset Attributes\n",
    "ds_Zarr.attrs = {\n",
    "'comment':                         'Weekly mean sea ice concentration forecasted by multiple models as well as observed by remotly sensed passive microwave sensors.',\n",
    "'contact':                         'nicway@uw.edu',\n",
    "'creator_email':                   'nicway@uw.edu',\n",
    "'creator_name':                    'Nicholas Wayand, University of Washington',\n",
    "'creator_url':                     'https://atmos.uw.edu/sipn/',\n",
    "'date_created':                    '2018-12-03T00:00:00',\n",
    "'date_modified':                   datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "'geospatial_lat_max':              str(float(ds_Zarr.lat.max().values)),\n",
    "'geospatial_lat_min':              str(float(ds_Zarr.lat.min().values)),\n",
    "'geospatial_lat_resolution':       '~25km',\n",
    "'geospatial_lat_units':            'degrees_north',\n",
    "'geospatial_lon_max':              str(float(ds_Zarr.lon.max().values)),\n",
    "'geospatial_lon_min':              str(float(ds_Zarr.lon.min().values)),\n",
    "'geospatial_lon_resolution':       '~25km',\n",
    "'geospatial_lon_units':            'degrees_east',\n",
    "'history':                         datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')+': updated by Nicholas Wayand',\n",
    "'institution':                     'UW, SIPN, ARCUS',\n",
    "'keywords':                        'Arctic > Sea ice concentration > Prediction',\n",
    "'product_version':                 '1.0',\n",
    "'project':                         'Sea Ice Prediction Network Phase II',\n",
    "'references':                      'Wayand, N.E., Bitz, C.M., and E. Blanchard-Wrigglesworth, (in review). A year-round sub-seasonal to seasonal sea ice prediction portal. Submited to Geophysical Research letters.',\n",
    "'source':                          'Numerical model predictions and Passive microwave measurments.',\n",
    "'summary':                         'Dataset is updated daily with weekly sea ice forecasts',\n",
    "'time_coverage_end':               pd.to_datetime(ds_Zarr.valid_end.max().values).strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "'time_coverage_start':             pd.to_datetime(ds_Zarr.init_start.min().values).strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "'title':                           'SIPN2 Sea ice Concentration Forecasts and Observations.'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hack to decode strings\n",
    "# ds_Zarr['model'] = [s.decode(\"utf-8\") for s in ds_Zarr.model.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to one Big Zarr\n",
    "ds_Zarr.to_zarr(os.path.join(E.data_dir,'model/zarr', cvar+'.zarr'), mode='w')\n",
    "print(\"Saved to Large Zarr.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.6.4 esio",
   "language": "python",
   "name": "esio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
